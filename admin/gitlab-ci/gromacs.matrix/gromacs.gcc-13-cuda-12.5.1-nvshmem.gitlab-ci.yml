# Test goal: CUDA GPU communications with OpenMPI,NVSHMEM
# Test intents (should use as recent as possible dependencies):
#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
#   GPU: NVHPC SDK newest supported with its newest supported CUDA and gcc
#   HW: dual NVIDIA GPU (CC 7.0 or newer required)
#   MPI: OpenMPI
#   Features: NVSHMEM, GPU direct communications (manual) + update (regression tests with dual GPU)
#   Scope: configure, build, regression tests
# Test implementation choices (free to change as needed):
#   OS: Ubuntu 22.04
#   Build type: RelWithDebInfo
#   Compiler: GCC 13.1
#   GPU: NVHPC SDK 24.7, CUDA 12.5.1
#   SIMD: SSE 4.1
#   FFT: FFTW3
#   Parallelism np/ntomp: 4/1 (regression tests with dual GPU)

gromacs:gcc-13-cuda-12.5.1:configureMPI-NVSHMEM:
  extends:
    - .gromacs:base:configure
    - .use-gcc:base
    - .use-cuda
    - .use-mpi
  rules:
    - !reference [.rules:post-merge-acceptance, rules]
  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1-openmpi-5.0.3
  variables:
    CMAKE: /usr/local/cmake-3.29.8/bin/cmake
    CMAKE_SIMD_OPTIONS: "-DGMX_SIMD=SSE4.1"
    CMAKE_EXTRA_OPTIONS: "-DGMX_NVSHMEM=ON -DNVSHMEM_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem"
    COMPILER_MAJOR_VERSION: 13

gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM:
  extends:
    - .gromacs:base:build
    - .before_script:default
    - .use-ccache
  rules:
    - !reference [.rules:post-merge-acceptance, rules]
  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1-openmpi-5.0.3
  variables:
    CMAKE: /usr/local/cmake-3.29.8/bin/cmake
    CMAKE_EXTRA_OPTIONS: "-DGMX_NVSHMEM=ON -DNVSHMEM_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem"
    # Workaround because the CMake support for NVSHMEM needs improvement:
    LD_LIBRARY_FLAGS: /opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem
  needs:
    - job: gromacs:gcc-13-cuda-12.5.1:configureMPI-NVSHMEM

gromacs:gcc-13-cuda-12.5.1:testMPI-NVSHMEM:
  extends:
    - .gromacs:base:test
  rules:
    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
    - !reference [.rules:post-merge-acceptance, rules]
  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1-openmpi-5.0.3
  variables:
    CMAKE: /usr/local/cmake-3.29.8/bin/cmake
    GMX_ENABLE_NVSHMEM: 1
    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
    NVSHMEM_DISABLE_NCCL: 1
    GMX_TEST_REQUIRED_NUMBER_OF_DEVICES: 2
    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
    GPU_VENDOR: "NVIDIA"
    GPU_COUNT: 2
  tags:
    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
  needs:
    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM

gromacs:gcc-13-cuda-12.5.1:regressiontest-gpucommupd-MPI-NVSHMEM:
  # Test parallelism np/ntomp: 4/1
  # Test parallelism GPU: direct communications, update
  extends:
    - .gromacs:base:regressiontest
  rules:
    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
    - !reference [.rules:post-merge-acceptance, rules]
  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1-openmpi-5.0.3
  variables:
    CMAKE: /usr/local/cmake-3.29.8/bin/cmake
    GMX_ENABLE_NVSHMEM: 1
    # Test 1 PME - 1 PP rank runs in regressiontests
    # as nvshmem support is for PME to PP force transfers
    REGRESSIONTEST_PME_RANK_NUMBER: 1
    REGRESSIONTEST_TOTAL_RANK_NUMBER: 2
    REGRESSIONTEST_OMP_RANK_NUMBER: 1
    REGRESSIONTEST_PARALLEL: "-np"
    GMX_TEST_REQUIRED_NUMBER_OF_DEVICES: 2
    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
    NVSHMEM_DISABLE_NCCL: 1
    GPU_VENDOR: "NVIDIA"
    GPU_COUNT: 2
  tags:
    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
  needs:
    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM
    - job: regressiontests:prepare
  artifacts:
    paths:
      - regressiontests
    when: always
    expire_in: 1 week

gromacs:gcc-13-cuda-12.5.1:regressiontest-gpucommupd-RF-MPI-NVSHMEM:
  # Test parallelism np/ntomp: 2/1
  # Test parallelism GPU: direct communications, update, NVSHMEM
  extends:
    - .gromacs:base:regressiontest
  rules:
    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
    - !reference [.rules:post-merge-acceptance, rules]
  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1-openmpi-5.0.3
  variables:
    CMAKE: /usr/local/cmake-3.29.8/bin/cmake
    GMX_ENABLE_NVSHMEM: 1
    # Test 2 PP rank runs in regressiontests for NVSHMEM enabled PP-PP haloexchange
    REGRESSIONTEST_PME_RANK_NUMBER: 0
    REGRESSIONTEST_TOTAL_RANK_NUMBER: 2
    REGRESSIONTEST_OMP_RANK_NUMBER: 1
    REGRESSIONTEST_PARALLEL: "-np"
    GMX_TEST_REQUIRED_NUMBER_OF_DEVICES: 2
    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
    NVSHMEM_DISABLE_NCCL: 1
    GPU_VENDOR: "NVIDIA"
    GPU_COUNT: 2
  tags:
    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
  needs:
    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM
    - job: regressiontests:prepare
  artifacts:
    paths:
      - regressiontests
    when: always
    expire_in: 1 week
