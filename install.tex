%
% $Id$
% 
%       This source code is part of
% 
%        G   R   O   M   A   C   S
% 
% GROningen MAchine for Chemical Simulations
% 
%               VERSION 2.0
% 
% Copyright (c) 1991-1999
% BIOSON Research Institute, Dept. of Biophysical Chemistry
% University of Groningen, The Netherlands
% 
% Please refer to:
% GROMACS: A message-passing parallel molecular dynamics implementation
% H.J.C. Berendsen, D. van der Spoel and R. van Drunen
% Comp. Phys. Comm. 91, 43-56 (1995)
% 
% Also check out our WWW page:
% http://md.chem.rug.nl/~gmx
% or e-mail to:
% gromacs@chem.rug.nl
% 
% And Hey:
% Gnomes, ROck Monsters And Chili Sauce
%

\chapter{Technical Details}
\label{ch:install}
\section{Installation}
The entire {\gromacs} package is Free Software, licensed under the GNU
General Public License. The main distribution site is our WWW server at
{\wwwpage}. 

The package is mainly distributed as source code, but we also provide
RPM packages for Linux.
On the home page you will find all the information you need to 
\normindex{install} the package, mailing lists with archives,
and several additional online resources like contributed topologies, etc.
The default installation action is simply to unpack the source code and
the issue
\begin{verbatim}
./configure
make
make install
\end{verbatim}
The configuration script should automatically determine the best options
for your platform, and it will tell you if anything is missing on
your system. You will also find detailed step-by-step installation
instructions on the website.

\section{Single or Double precision}
{\gromacs} can be compiled in either single\index{single
precision|see{precision, single}}\index{precision, single}
or \pawsindex{double}{precision}. The default choice is single
precision, but it is easy to turn on double precision by selecting
the {\tt --disable-float} option to the configuration script.
Double precision will be 0 to 50\% slower than single precision depending
on the architecture you are running on. Double precision will use somewhat more
memory and run input, energy and full-precision trajectory files will be
almost twice as large. Note that the assembly loops are only available in
single precision; Although the Intel SSE2 instruction set (available
on Pentium IV and later) supports double precision instructions the performance
is much lower than single precision. It would also mean very much extra
work for a feature that very few people use, so we will probably not 
provide double precision assembly loops in the future either.

The energies in single precision are accurate up to the last decimal,
the last one or two decimals of the forces are non-significant.
The virial is less accurate than the forces, since the virial is only one
order of magnitude larger than the size of each element in the sum over
all atoms (\secref{virial}).
In most cases this is not really a problem, since the fluctuations in de
virial can be 2 orders of magnitude larger than the average.
In periodic charged systems these errors are often negligible.
Especially cut-off's for the Coulomb interactions cause large errors
in the energies, forces and virial.
Even when using a reaction-field or lattice sum method the errors
are larger than or comparable to the errors due to the single precision.
Since MD is chaotic, trajectories with very similar starting conditions will
diverge rapidly, the divergence is faster in single precision than in double
precision.

For most simulations single precision is accurate enough.
In some cases double precision is required to get reasonable results:
\begin{itemize}
\item normal mode analysis,
for the conjugate gradient or l-bfgs minimization and the calculation and
diagonalization of the Hessian
\item calculation of the constraint force between two large groups of atoms
\item energy conservation (this can only be done without temperature coupling
and without cut-off's)
\end{itemize}

\section{Porting {\gromacs}}
The {\gromacs} system is designed with portability as a major design
goal. However there are a number of things we assume to be present on
the system {\gromacs} is being ported on. We assume the following
features:

\begin{enumerate}
\item   A UNIX-like operating system (BSD 4.x or SYSTEM V rev.3 or higher) 
        or UNIX-like libraries running under e.g. CygWin
\item   an ANSI C compiler 
\item   optionally a Fortran-77 compiler or Fortran-90 compiler
        for faster (on some computers) inner loop routines
\item   optionally the Nasm assembler to use the assembly innerloops
        on x86 processors.
\end{enumerate}

There are some additional features in the package that require extra
stuff to be present, but it is checked for in the configuration script
and you will be warned if anything important is missing.

That's the requirements for a single processor system. If you want
to compile {\gromacs} for a multiple processor environment you also
need a MPI library (Message-Passing Interface) to perform the 
parallel communication. This is always shipped with supercomputers, and
for workstations you can find links to free MPI implementations through
the {\gromacs} homepage at {\wwwpage}.

\subsection{Multi-processor Optimization}

If you want to, you could write your own optimized communication
(perhaps using specific libraries for your hardware) instead
of MPI. This should never be necessary for normal use
(we haven't heard of a modern computer where it isn't possible
to run MPI), but if you absolutely want to do it, here are some clues.

The interface between the communication routines and the
rest of the {\gromacs} system is described in the file {\tt
\$GMXHOME/src/include/network.h} We will give a short description of the
different routines below.

\begin{description}
\item[{\bf extern void gmx\_tx(int pid,void *buf,int bufsize);}]\mbox{}\\ 
This routine, when called with the destination processor number, a
pointer to a (byte oriented) transfer buffer, and the size of the
buffer will send the buffer to the indicated processor (in our case
always the neighboring processor). The routine does {\bf not} wait
until the transfer is finished.

\item[{\bf extern void gmx\_tx\_wait(int pid);}]\mbox{}\\
This routine waits until the previous, or the ongoing transmission is
finished.

\item[{\bf extern void gmx\_txs(int pid,void *buf,int bufsize);}]\mbox{}\\
This routine implements a synchronous send by calling the
a-synchronous routine and then the wait. It might come in handy to
code this differently.

\item[{\bf extern void gmx\_rx(int pid,void *buf,int bufsize);}]
\item[{\bf extern void gmx\_rx\_wait(int pid);}]\vspace{-\itemsep}
\item[{\bf extern void gmx\_rxs(int pid,void *buf,int bufsize);}]\vspace{-\itemsep}\mbox{}\\
The very same routines for receiving a buffer and waiting until the
reception is finished.

\item[{\bf extern void gmx\_init(int pid,int nprocs);}]\mbox{}\\
This routine initializes the different devices needed to do the
communication. In general it sets up the communication hardware (if it
is accessible) or does an initialize call to the lower level
communication subsystem.

\item[{\bf extern void gmx\_stat(FILE *fp,char *msg);}]\mbox{}\\
With this routine we can diagnose the ongoing communication. In the
current implementation it prints the various contents of the hardware
communication registers of the (\intel) multiprocessor boards to a
file.
\end{description}

\section{Environment Variables}
{\gromacs} programs may be influenced by the use of 
\normindex{environment variables}. 
First of all, the variables set in the \normindex{GMXRC} file
are essential for running and compiling {\gromacs}. Other variables are:
\begin{enumerate}
\item   {\tt DUMP\_NL}, dump neighbor list. 
        If set to a positive number the {\em entire}
        neighbor list is printed in the log file (may be many megabytes).
        Mainly for debugging purposes, but may also be handy for
        porting to other platforms.
\item   {\tt GMX\_NO\_QUOTES}, if this is explicitly set, no cool quotes
        will be printed at the end of a program
\item   {\tt WHERE}, when set print debugging info on line numbers.
\item   {\tt LOG\_BUFS}, the size of the buffer for file I/O. When set
        to 0, all file I/O will be unbuffered and therefore very slow.
        This can be handy for debugging purposes, because it ensures
        that all files are always totally up-to-date.
\item   {\tt GMXNPRI}, for SGI systems only. When set, gives the
        default non-degrading priority (npri) for {\tt
        mdrun}, {\tt nmrun}, {\tt g\_covar} and {\tt g\_nmeig},
        {\eg}\@ setting \verb'setenv GMXNPRI 250' causes all
        runs to be performed at near-lowest priority by default.
\item   {\tt GMX\_VIEW\_XPM}, {\tt GMX\_VIEW\_XVG}, {\tt
        GMX\_VIEW\_EPS} and {\tt GMX\_VIEW\_PDB}, commands used to
        automatically view resp.\@ {\tt .xvg}, {\tt .xpm}, {\tt .eps}
        and {\tt .pdb} file types; they default to {\tt xv}, {\tt xmgrace},
        {\tt ghostview} and {\tt rasmol}. Set to empty to disable
        automatic viewing of a particular file type. The command will
        be forked off and run in the background at the same priority
        as the {\gromacs} tool (which might not be what you want).
        Be careful not to use a command which blocks the terminal
        (e.g. {\tt vi}), since multiple instances might be run.
\end{enumerate}

Some other environment variables are specific to one program, such as
TOTAL for the {\tt \normindex{do\_shift}} program, and DSPP for the
{\tt \normindex{do\_dssp}} program.

\section{Running {\gromacs} in parallel}
If you have installed the MPI (Message Passing Interface) on your computer(s)
you can compile {\gromacs} with this library to run simulations in parallel. 
All supercomputers are shipped with MPI libraries optimized for 
that particular platform, and if you are using a cluster of workstations
there are several good free MPI implementations. You can find updated links
to these on the gromacs homepage {\wwwpage}. Once you have an MPI library
installed it's trivial to compile {\gromacs} with MPI support: Just set
the option {\tt --enable-mpi} to the configure script and recompile.
(But don't forget to make distclean before running configure if you have
previously compiled with a different configuration.) If you are using a 
supercomputer you might also want to turn of the default nicing of the
mdrun process with the {\tt --disable-nice} option.

There is usually a program called {\tt mpirun} with which you can fire
up the parallel processes. A typical command line looks like:
\type{mpirun -p goofus,doofus,fred 10 mdrun -s topol -v -N 30}
this runs on each of the machines goofus,doofus,fred with 10 processes
on each\footnote{Example taken from Silicon Graphics manual}.

If you have a single machine with multiple processors you don't have to
use the {\tt mpirun} command, but you can do with an extra option to
{\tt mdrun}:
\type{mdrun -np 8 -s topol -v -N 8}
In this example MPI reads the first option from the command line.
Since {\tt mdrun} also wants to know the number of processes you have to
type it twice.

Check your local manuals (or online manual) for exact details
of your MPI implementation.

If you are interested in programming MPI yourself, you can find
manuals and reference literature on the internet.


