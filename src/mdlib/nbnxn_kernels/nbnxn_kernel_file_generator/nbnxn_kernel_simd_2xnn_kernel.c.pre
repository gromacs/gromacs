#include "{4}"

{1}
{2}
{3}

#ifdef {0}

#include "gmx_simd_macros.h"
#include "gmx_simd_vec.h"
#include "../../nbnxn_consts.h"
#ifdef CALC_COUL_EWALD
#include "maths.h"
#endif

/* Define a few macros for half-width SIMD */
#if defined GMX_X86_AVX_256 && !defined GMX_DOUBLE

/* Half-width SIMD real type */
#define gmx_mm_hpr  __m128

/* Half-width SIMD operations */
/* Load reals at half-width aligned pointer b into half-width SIMD register a */
#define gmx_load_hpr(a, b)    a = _mm_load_ps(b)
/* Load one real at pointer b into half-width SIMD register a */
#define gmx_load1_hpr(a, b)   a = _mm_load1_ps(b)
/* Load one real at b and one real at b+1 into halves of a, respectively */
#define gmx_load1p1_pr(a, b)  a = _mm256_insertf128_ps(_mm256_castps128_ps256(_mm_load1_ps(b)), _mm_load1_ps(b+1), 0x1)
/* Load reals at half-width aligned pointer b into two halves of a */
#define gmx_loaddh_pr(a, b)   a = gmx_mm256_load4_ps(b)
/* To half-width SIMD register b into half width aligned memory a */
#define gmx_store_hpr(a, b)       _mm_store_ps(a, b)
#define gmx_add_hpr               _mm_add_ps
#define gmx_sub_hpr               _mm_sub_ps
/* Horizontal sum over a half SIMD register */
#define gmx_sum4_hpr              gmx_mm256_sum4h_m128

#else
#error "Half-width SIMD macros are not yet defined"
#endif


#define SUM_SIMD4(x) (x[0]+x[1]+x[2]+x[3])

#define UNROLLI    NBNXN_CPU_CLUSTER_I_SIZE
#define UNROLLJ    (GMX_SIMD_WIDTH_HERE/2)

/* The stride of all the atom data arrays is equal to half the SIMD width */
#define STRIDE     (GMX_SIMD_WIDTH_HERE/2)

#if GMX_SIMD_WIDTH_HERE == 8
#define SUM_SIMD(x) (x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6]+x[7])
#else
#if GMX_SIMD_WIDTH_HERE == 16
/* This is getting ridiculous, SIMD horizontal adds would help,
 * but this is not performance critical (only used to reduce energies)
 */
#define SUM_SIMD(x) (x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6]+x[7]+x[8]+x[9]+x[10]+x[11]+x[12]+x[13]+x[14]+x[15])
#else
#error "unsupported kernel configuration"
#endif
#endif


#if defined GMX_X86_AVX_256 && !defined GMX_DOUBLE
/* AVX-256 single precision 2x(4+4) kernel,
 * we can do half SIMD-width aligned FDV0 table loads.
 */
#define TAB_FDV0
#endif

/* Currently stride 4 for the 2 LJ parameters is hard coded */
#define NBFP_STRIDE  4


#define SIMD_MASK_ALL   0xffffffff

#include "../nbnxn_kernel_simd_utils.h"

/* All functionality defines are set here, except for:
 * CALC_ENERGIES, ENERGY_GROUPS which are defined before.
 * CHECK_EXCLS, which is set just before including the inner loop contents.
 * The combination rule defines, LJ_COMB_GEOM or LJ_COMB_LB are currently
 * set before calling the kernel function. We might want to move that
 * to inside the n-loop and have a different combination rule for different
 * ci's, as no combination rule gives a 50% performance hit for LJ.
 */

/* We always calculate shift forces, because it's cheap anyhow */
#define CALC_SHIFTFORCES

/* Assumes all LJ parameters are identical */
/* #define FIX_LJ_C */
#endif /* {0} */

#ifdef CALC_ENERGIES
void
{5}(const nbnxn_pairlist_t     *nbl,
{6}const nbnxn_atomdata_t     *nbat,
{6}const interaction_const_t  *ic,
{6}rvec                       *shift_vec,
{6}real                       *f,
{6}real                       *fshift,
{6}real                       *Vvdw,
{6}real                       *Vc)
#else
void
{5}(const nbnxn_pairlist_t     *nbl,
{6}const nbnxn_atomdata_t     *nbat,
{6}const interaction_const_t  *ic,
{6}rvec                       *shift_vec,
{6}real                       *f,
{6}real                       *fshift)
#endif
#ifdef {0}
#include "nbnxn_kernel_simd_2xnn_outer.h"
#else /* {0} */
{{
/* This function can never be called, but since the code generation
 * takes place before CMake is called, and only at CMake time do we
 * know which nbnxn kernels will be used on the target architecture,
 * there has to be a stub function definition.
 */
}}
#endif /* {0} */
